---
title: "Penstemon penlandii"
author: "Michelle DePrenger-Levin"
date: "2023-02-11"
output: html_document
---

Fay et al 2021 example Mixture of Bernoulli- and Poisson-distributed demographic parameters    
```{r}
### Simulation of the individual life-histories
library(mvtnorm)
library(R2jags)
library(jagsUI)
library(runjags)
library(mcmcplots)
library(boot)


ilogit <- function(x) 1/(1+exp(-x))
logit <- function(x) log(x/(1-x))
# Parameters of the simulated data
n.released <- 25 # number of individuals marked and released per year
n.year <- 20 # number of years
n.ind <- n.released * n.year # Total number of individuals in the dataset
mu.phi <- 0.5 # Survival probability
mu.psi <- 0.7 # Breeding success probability
mu.p <- 0.5 # Detection probability
sigma.alpha.phi <- 0.2 # Among-individual variation in survival
sigma.alpha.psi <- 0.2 # Among-individual variation in reproductive success

cor.alpha <- 0.6 # Correlation between survival and breeding individual variation
sigma.eps.phi <- 0.5 # Survival temporal variation
sigma.eps.psi <- 0.5 # Breeding success temporal variation

cor.eps <- 0 # Correlation between survival and breeding temporal variation
gamma.phi <- - logit(0.4) # State effect for survival
gamma.psi <- logit(0.7)-logit(0.6) # State effect for reproductive success
n.loop <- 1
# Objects to record the results
RESULT <- array(NA, dim = c(11,2,n.loop))
rownames(RESULT) = c('mean.phi', 'mean.psi', 'mean.psi.re', 'mean.p', 'cor.alpha', 'sigma.alpha.phi', 'sigma.alpha.psi',
                     'sigma.eps.phi', 'sigma.eps.psi', 'gamma.phi', 'gamma.psi')

## fill this in twice, make a couple sets I guess
for (k in 1:n.loop){
  ## Simulation of individual variation
  alpha <- array(NA, dim = c(n.ind,2))
  # Variance covariance matrix
  varcov.alpha <- array(NA, dim = c(2,2))
  varcov.alpha[1,1] <- sigma.alpha.phi^2
  varcov.alpha[2,2] <- sigma.alpha.psi^2
  varcov.alpha[1,2] <- varcov.alpha[2,1] <- cor.alpha * sigma.alpha.phi * sigma.alpha.psi ## diagonal, 
  alpha <- rmvnorm(n.ind, c(0,0), varcov.alpha)
  ## Simulation of temporal variation
  epsilon <- array(NA, dim = c(n.year,2))
  # Variance covariance matrix
  varcov.eps <- array(NA, dim = c(2,2))
  varcov.eps[1,1] <- sigma.eps.phi^2
  varcov.eps[2,2] <- sigma.eps.psi^2
  varcov.eps[1,2] <- varcov.eps[2,1] <- cor.eps * sigma.eps.phi * sigma.eps.psi
  epsilon <- rmvnorm(n.year, c(0,0), varcov.eps)
  ## Simulation of the breeding process: Successful (1), Failed (2)
  mat.breed <- array(NA, dim = c(n.released*n.year, n.year)) # Breeding success matrix
  # Year and individual specific breeding success probabilities
  breed.prob <- array(NA, dim = c(n.ind, n.year))
  
  # a new column for each year, row for each individual with another 25 added each year
  for (t in 1:n.year){
    for (i in 1:(t*n.released)){
      breed.prob[i,t] <- logit(mu.psi) + alpha[i,2] + epsilon[t,2]
    }
  }
  
  # first year for each 25 per year
  for (t in 1:n.year){
    for (i in (1+(t-1)*n.released):(t*n.released)){
      mat.breed[i,t] <- rbinom(1,1,ilogit(breed.prob[i,t] - gamma.psi)) + 1
    }
  }
  
  # following years
  for (t in 2:n.year){
    for (i in 1:((t-1)*n.released)){
      state <- ifelse(mat.breed[i,t-1]==2,0,1)
      mat.breed[i,t] <- rbinom(1,1,ilogit(breed.prob[i,t] - state * gamma.psi)) + 1
    }
  }

  ## Simulation of the survival process
  mat.s <- array(NA, dim = c(n.ind, n.year)) # Survival matrix
  # Year and individual specific survival probabilities
  s.prob <- array(NA, dim = c(n.ind, n.year))
  for (t in 2:n.year){
    for (i in 1:((t-1)*n.released)){
      s.prob[i,t] <- logit(mu.phi) + alpha[i,1] + epsilon[t,1]
    }
  }
  
  # Alive at the first capture
  for (t in 1:n.year){
    mat.s[(1+(t-1)*n.released):(t*n.released),t] <- 1
    }
  # After the first capture
  for (t in 2:n.year){
    for (i in 1:((t-1)*n.released)){
      state <- ifelse(mat.breed[i,t-1]==2,0,1)
      mat.s[i,t] <- rbinom(1,1,ilogit(s.prob[i,t] - state * gamma.phi)*mat.s[i,t-1])
    }
  }
  
  ## Simulation of the observation process
  mat.obs <- array(1, dim = c(n.released*n.year, n.year)) # Observation matrix
  # After the first capture
  for (t in 2:n.year){
    for (i in 1:((t-1)*n.released)){
      mat.obs[i,t] <- rbinom(1,1,mu.p)
    }
  }
  
  ## Final dataset
  data <- mat.s * mat.breed * mat.obs
  # At which occasion the first capture occur
  get.first <- function(x) min(which(x!=0))
  first <- apply(data, 1, get.first)
  # The last capture occasion
  last <- rep(n.year,n.ind)
  # Build datasets to feed jags

  SURVIVAL <- mat.s * mat.obs
  SURVIVAL[data==0] <- NA
  BREED <- data
  BREED[data==0] <- NA ; BREED[data==1] <- 0 ; BREED[data==2] <- 1
  DETECTED <- mat.obs * mat.s
  
  # Initial values of latent variables to run the model
  mat.surv <- array(NA, dim = c(n.ind,n.year))
  breed <- array(NA, dim = c(n.ind,n.year))
  for (i in 1:n.ind){
    mat.surv[i, first[i]:last[i]] <- 1
    breed[i, first[i]:last[i]] <- rbinom(length(first[i]:last[i]), size = 1, prob = 0.5)
    }
  init.surv <- mat.surv * ifelse(data == 0, 1, NA)
  init.breed <- breed * ifelse(is.na(BREED), 1, NA)

### JAGS model
cat(file = "model01.jags", "
  model {
  ### DATA
  # SURVIVAL[i, t] with NA, 1 and 0
  # BREED[i, t] with NA, 1 and 0
  # DETECTED[i, t] with 0 and 1
  # Pointers at the individual level
  # FIRST[i]:LAST[i]
  # n.year
  # n.ind
  ### LIKELIHOOD ###
  for(i in 1:n.ind) {
  ## Initial conditions
  SURVIVAL[i, FIRST[i]] ~ dbern(1.0)
  BREED[i, FIRST[i]] ~ dbern(psi[i, FIRST[i]] * SURVIVAL[i, FIRST[i]])
  psi[i, FIRST[i]] <- ilogit(mu.psi.re + alpha[i, 2] + epsilon[FIRST[i], 2])
  # note that you're necessarily detected on first occasion
  DETECTED[i, FIRST[i]] ~ dbern(1.0)
  ## Rest of the life histories
  for(t in (FIRST[i] + 1):LAST[i]) {
  # survival
  SURVIVAL[i, t] ~ dbern(surv[i, t] * SURVIVAL[i, t-1])
  surv[i, t] <- ilogit(mu.phi + gamma[1] * BREED[i, t-1] + alpha[i, 1] + epsilon[t, 1])
  # breed
  BREED[i, t] ~ dbern(psi[i, t] * SURVIVAL[i, t])
  psi[i, t] <- ilogit(mu.psi + gamma[2] * BREED[i, t-1] + alpha[i, 2] + epsilon[t, 2])
  # detection: you can only be detected if you're currently alive
  # distinguish between states: breeding and non breeding
  DETECTED[i, t] ~ dbern(ilogit(mu.p) * SURVIVAL[i, t])
  }
  }
  ### PRIORS ###
  # survival
  mu.phi ~ dnorm(0.0, 0.4444444)
  # breeding
  mu.psi ~ dnorm(0.0, 0.4444444)
  mu.psi.re ~ dnorm(0.0, 0.4444444)
  # detection
  mu.p ~ dnorm(0.0, 0.4444444)
  # state dependent effects
  prec_gamma <- pow(log(2)/2, -2);
  for (j in 1:2) {
    gamma[j] ~ dnorm(0.0, prec_gamma);
  }
  # Individual effect
  # Cholesky decomposition with parameter expansion
  for (j in 1:2) {
    A[j, j] ~ dnorm(0.0, 0.4444444)T(0.0,)
    DeltaA[j, j] <- 1/tauA[j] ; tauA[j] ~ dgamma(1.5, 1.5);
    LA[j, j] <- 1.0;
  }
  LA[1, 2] <- 0.0; A[1, 2] <- 0.0; DeltaA[1, 2] <- 0.0;
  LA[2, 1] ~ dnorm(0.0, 4.0); A[2, 1] <- 0.0; DeltaA[2, 1] <- 0.0;
  # covariance matrix
  Omega <- A %*% LA %*% DeltaA %*% t(LA) %*% A;
  
  for(i in 1:n.ind){
    alpha[i, 1] <- A[1, 1] * (LA[1, 1] * xi_a[i, 1]);
    alpha[i, 2] <- A[2, 2] * (LA[2, 1] * xi_a[i, 1] + LA[2, 2] * xi_a[i, 2]);
    for(j in 1:2){
        xi_a[i, j] ~ dnorm(0.0, tauA[j]);
      } 
  }
  sigma.alpha.phi <- sqrt(Omega[1, 1])
  sigma.alpha.psi <- sqrt(Omega[2, 2])
  cor.alpha <- Omega[1, 2] / sqrt(Omega[1, 1] * Omega[2, 2])
  # Environmental effect
    for (j in 1:2) {
      E[j, j] ~ dnorm(0.0, 0.4444444)T(0.0,)
      DeltaE[j, j] <- 1/tauE[j] ; tauE[j] ~ dgamma(1.5, 1.5);
      LE[j, j] <- 1.0;
    }
  LE[1, 2] <- 0.0; E[1, 2] <- 0.0; DeltaE[1, 2] <- 0.0;
  LE[2, 1] ~ dnorm(0.0, 4.0); E[2, 1] <- 0.0; DeltaE[2, 1] <- 0.0;
  
  # covariance matrix
  Lambda <- E %*% LE %*% DeltaE %*% t(LE) %*% E;
  
  ## for a new for each year and each xi_e
  for(i in 1:n.year){
    epsilon[i, 1] <- E[1, 1] * (LE[1, 1] * xi_e[i, 1]);
    epsilon[i, 2] <- E[2, 2] * (LE[2, 1] * xi_e[i, 1] + LE[2, 2] * xi_e[i, 2]);
      for(j in 1:2){
        xi_e[i, j] ~ dnorm(0.0, tauE[j]);
      }
  }
  sigma.eps.phi <- sqrt(Lambda[1, 1])
  sigma.eps.psi <- sqrt(Lambda[2, 2])
  cor.eps <- Lambda[1, 2] / sqrt(Lambda[1, 1] * Lambda[2, 2])
  # Derived quantity
  mean.phi <- ilogit(mu.phi)
  mean.psi.re <- ilogit(mu.psi.re)
  mean.psi <- ilogit(mu.psi)
  mean.p <- ilogit(mu.p)
}
")
  
# Bundle data
jags.data <- list(SURVIVAL = SURVIVAL, BREED = BREED, FIRST = first, LAST = last, n.ind = n.ind, n.year = n.year)
inits <- function(){list(SURVIVAL = init.surv, BREED = init.breed)}
# Parameters monitored
parameters <- c("mean.phi", "mean.psi", "mean.psi.re", "mean.p", "cor.alpha", "sigma.alpha.phi", "sigma.alpha.psi",
                "sigma.eps.phi", "sigma.eps.psi", "gamma")
# MCMC settings
ni <- 8000
nt <- 4
nb <- 2000
nc <- 3
# Call JAGS from R (BRT 1 min)
out1 <- jags(jags.data, inits, parameters, "model01.jags", n.chains = nc, n.thin = nt, n.iter = ni, 
             n.burnin = nb, parallel = TRUE)
RESULT[1:11,1,k] <- c(out1$mean$mean.phi, out1$mean$mean.psi, out1$mean$mean.psi.re, out1$mean$mean.p, 
                      out1$mean$cor.alpha, out1$mean$sigma.alpha.phi, out1$mean$sigma.alpha.psi, 
                      out1$mean$sigma.eps.phi, out1$mean$sigma.eps.psi, out1$mean$gamma[1], out1$mean$gamma[2])
RESULT[1:11,2,k] <- c(out1$Rhat$mean.phi, out1$Rhat$mean.psi, out1$Rhat$mean.psi.re, out1$Rhat$mean.p, 
                      out1$Rhat$cor.alpha, out1$Rhat$sigma.alpha.phi, out1$Rhat$sigma.alpha.psi, 
                      out1$Rhat$sigma.eps.phi, out1$Rhat$sigma.eps.psi, out1$Rhat$gamma[1], out1$Rhat$gamma[2])
}

print(out1, digits = 3)
# trace plots
traceplot(out1,c("mean.phi", "mean.psi","mean.psi.re","mean.p","cor.alpha"))
densityplot(out1)

```



McElreath mom-daughter example 
```{r}
set.seed(1908)
N <- 200 # number of pairs
U <- rnorm(N) # simulate confounds, normal 0 mean 1 SD
# birth order and family sizes
B1 <- rbinom(N,size=1,prob=0.5) # 50% first borns
M <- rnorm( N , 2*B1 + U ) ## Mom's family size: so first born end up with 2 + confound error if mom is first born, just confound if not first born
B2 <- rbinom(N,size=1,prob=0.5) # Daughter's birth order
D <- rnorm( N , 2*B2 + U + 0*M ) # change the 0 to turn on causal influence of mom

# fit the two regression models
summary( lm( D ~ M ) )
summary( lm( D ~ M + B1 + B2 ) )

# compare the models with AIC
AIC( lm( D ~ M ) )
AIC( lm( D ~ M + B1 + B2 ) )
```


Olivier Gimenez <https://oliviergimenez.github.io/blog/sim_with_jags/>    

model block of JAGS can work in 'reverse' to generate data based on fixed parameters, specify the parameters as 'data' to JAGS, monitor the simulated data points  
'data block' updated once at start of simulation  
'model block' updated at each iteration so if need more than one, put in model block   


Are the parameters plugged into a data simulation process based on model reasonably recovered by an estimation process.   
response variable, y ~ a mean, mu for each data point, i   
mu[i] depends on the intercept, alpha, a parameter, beta, and the explanatory data variable for that parameter, x[i]  Example
```{r}
txtstring <- '
data{
# Likelihood:
for (i in 1:N){
y[i] ~ dnorm(mu[i], tau) # tau is precision (1 / variance)
mu[i] <- alpha + beta * x[i]
}
}
model{
fake <- 0
}
'
``` 


# Now fit the model used to simulate the data that was generated
```{r}
# specify model in BUGS language
modelpepe <- 	
paste("	
model {
    # Likelihood:
        for(i in 1:N){
          y[i] ~ dbern(theta[i]) ## theta: The probability of survival
          theta[i] <- alpha + beta * x[i]  
        }
    # Priors:
    alpha ~ dbeta(1, 1) # intercept
    beta ~ dnorm(0, 0.01) # slope
  }
")
writeLines(modelpepe,"lin_reg_pepe.jags")

# data, dat from simulation
jags.data <- list(y = dat, N = length(dat), x = x)

# initial values
inits <- function(){list(alpha = rbeta(1), beta = rnorm(1))}  

# parameters monitored
parameters <- c("alpha", "beta", "theta")

# MCMC settings
ni <- 10000
nt <- 6
nb <- 5000
nc <- 2

# call JAGS from R
res <- jags(jags.data, inits = NULL, parameters, "lin_reg_pepe.jags", n.chains = nc, n.thin = nt, 
            n.iter = ni, n.burnin = nb,
            working.directory = getwd()) 

print(res, digits = 3)

## Check convergence 
# trace plots
traplot(res,c("alpha", "beta"))
traplot(res,c("theta[1]"))
# posterior distributions
denplot(res,c("alpha", "beta", "theta[1]"))
```

  
```{r}
pepestring <- '
  data{
    # Likelihood:
      for(k in 1:TC){
        for(i in 1:N){
          y[i] ~ dbern(theta[k]) ## theta: The probability of survival
          theta[k] <- alpha + beta[k] * x[i]  ## one for each offset
        }
      }
    }
  model{
  fake <- 0
  }
  '

# parameters for simulations 
N = 400 # nb of observations
TC <- 4
x <- as.factor(rep(c("trSp","trFa","coSo","coDe"), each = 20)) # predictor
beta = c(-0.1,0.1,0.2,0.3) # offset of mean survival
theta <- .5 # probability of survival (variance is theta(1-theta))
# parameters are treated as data for the simulation step
data<-list(N=N,TC = TC,x=x,beta=beta,theta=theta)

# Run JAGS but monitor the response variable instead of the parameters because we're checking if the model works 
out <- run.jags(pepestring, data = data,monitor=c("y"),sample=1, n.chains=1, summarise=FALSE)
```




Set up the data to use in testing the model  
```{r}
# parameters for simulations 
N = 30 # nb of observations
x <- 1:N # predictor
alpha = 0.5 # intercept
beta = 1 # slope
sigma <- .1 # residual sd
tau <- 1/(sigma*sigma) # precision
# parameters are treated as data for the simulation step
data<-list(N=N,x=x,alpha=alpha,beta=beta,tau=tau)
```


Run JAGS but monitor the response variable instead of the parameters because we're checking if the model works  
```{r}
out <- run.jags(txtstring, data = data,monitor=c("y"),sample=1, n.chains=1, summarise=FALSE)
```

Format output 
```{r}
Simulated <- coda::as.mcmc(out)
## spit out y for each data point
dat <- as.vector(Simulated)
```

Now fit the model used to simulate the data that was generated
```{r}
# specify model in BUGS language
model <- 	
paste("	
model {
  # Likelihood:
  for (i in 1:N){
    y[i] ~ dnorm(mu[i], tau) # tau is precision (1 / variance)
    mu[i] <- alpha + beta * x[i]
    }
  # Priors:
  alpha ~ dnorm(0, 0.01) # intercept
  beta ~ dnorm(0, 0.01) # slope
  sigma ~ dunif(0, 100) # standard deviation
  tau <- 1 / (sigma * sigma) 
}
")
writeLines(model,"lin_reg.jags")	

# data, dat from simulation
jags.data <- list(y = dat, N = length(dat), x = x)

# initial values
inits <- function(){list(alpha = rnorm(1), beta = rnorm(1), sigma = runif(1,0,10))}  

# parameters monitored
parameters <- c("alpha", "beta", "sigma")

# MCMC settings
ni <- 10000
nt <- 6
nb <- 5000
nc <- 2

# call JAGS from R
res <- jags(jags.data, inits, parameters, "lin_reg.jags", n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb,
            working.directory = getwd()) 

print(res, digits = 3)

## Check convergence 
# trace plots
traplot(res,c("alpha", "beta", "sigma"))

# posterior distributions
denplot(res,c("alpha", "beta", "sigma"))
```

Determine if, with the uncertainty, can you get the parameters you specified? Yes 
## Capture-recapture example  
```{r}
## Simulate data
txtstring <- '
data{
# Constant survival and recapture probabilities
for (i in 1:nind){
   for (t in f[i]:(n.occasions-1)){
      phi[i,t] <- mean.phi
      p[i,t] <- mean.p
      } #t
   } #i
# Likelihood 
for (i in 1:nind){
   # Define latent state and obs at first capture
   z[i,f[i]] <- 1
   mu2[i,1] <- 1 * z[i,f[i]] # detection is 1 at first capture ("conditional on first capture")
   y[i,1] ~ dbern(mu2[i,1])
   # then deal w/ subsequent occasions
   for (t in (f[i]+1):n.occasions){
      # State process
      z[i,t] ~ dbern(mu1[i,t])
      mu1[i,t] <- phi[i,t-1] * z[i,t-1]
      # Observation process
      y[i,t] ~ dbern(mu2[i,t])
      mu2[i,t] <- p[i,t-1] * z[i,t]
      } #t
   } #i
}
model{
fake <- 0
}
'
```



######################################################################################
<https://mmeredith.net/blog/2019/partial_pooling.htm>   
We need to have separate R's for each of the sites, but each R must take account of what's happening at the other sites too, so the model is a good deal more sophisticated. In fact, it's a multi-level or hierarchical model. Let's show the JAGS code and then discuss the details:
```{r}
modeltxt <- "
data{
  for(i in 1:9) {
    # observation model
    y[i] ~ dbin(R[i], n[i])
    # prior for R[i]
    logitR[i] ~ dnorm(mu, tau)
    R[i] <- ilogit(logitR[i])  ## inverse logit probably
  }
  # hyperpriors for mu and tau
  # Rmean ~ dbeta(1, 1)
  # mu <- logit(Rmean)
  # sd ~ dunif(0, 10)
  # tau <- sd^-2
}
model{
fake <- 0
}"
writeLines(modeltxt, con="parpooling.jags")

# parameters for simulation 
mu <- logit(0.5) ## log(p)/log(1-p) 50:50 is  log(p)-log(1-5) which is zero!!!

```

<https://mmeredith.net/blog/2017/Categories_in_JAGS.htm>  
Categorical coefficients sum to 0
Choosing a reference category can make interpretation of the output difficult, especially if there are several categorical covariates, each with its own reference category. An alternative is to have a (non-zero) coefficient for each category but to ensure that they add up to 0 - or, if you prefer, the mean is 0. Here's the JAGS model:
```{r}
modelText <- "
model {
  for(i in 1:length(success)) {
    logit(p[i]) <- b0 + bPlayer[player[i]] + bFluff * fluff[i]
    success[i] ~ dbern(p[i])
  }

  # Priors
  b0 ~ dunif(-5,5)
  for(i in 1:4) {
    temp[i] ~ dnorm(0, 0.01)
  }
  bPlayer <- temp - mean(temp)
  bFluff ~ dunif(-5,5)
}"
writeLines(modelText, con="model3.txt")
``` 


Penstemon penlandii is a long-lived, iteroparous species. 

```{r}

rm(list=ls())
# remotes::install_github("jonesor/Rage")
library(Rage)
library(popdemo)
library(lefko3)
library(Rcompadre)

library(EnvStats)

library(dplyr)
library(tidyr)
library(purrr)
library(tidyverse)
library(patchwork)
library(DiagrammeR)
library(DiagrammeRsvg)
library(rsvg)

library(popbio)
library(MASS)


library(prism)
library(raster)

library(sp)
# prism_set_dl_dir("Q:/Research/All_Projects_by_Species/Astragalus SPECIES/Astragalus_microcymbus/PRISM_asmiclimate")

```


What is known about penstemons  
```{r}
## fetch most recent from website, add logical columns to metadata to flag potential problems
compadre <- cdb_fetch("compadre", flag = TRUE)

plants <- compadre %>%
  filter(MatrixSplit == "Divided") %>%
  filter(ProjectionInterval == 1) %>%
  filter(check_irreducible == TRUE) %>%
  subset(MatrixComposite != "Seasonal")

plants@data[grepl("Penstemon",plants@data$SpeciesAccepted),] ## none

plants[plants$SpeciesAccepted == "Digitalis purpurea",]$mat

```


Beta regression in JAGS   
To create b=betareg(p ~ id, data= d,    
          link = c("logit"), link.phi = NULL, type = c("ML"))   
summary(b)  
in JAGS   
```{r}
jags_str = "
model {
#model

for(i in 1:(len_id)){
  y[i] ~ dbeta(alpha[i], beta[i])
  alpha[i] <- mu[i] * phi
  beta[i]  <- (1-mu[i]) * phi
  logit(mu[i]) <- a + b*id[i]
}

#priors
a  ~ dnorm(0, .5)
b  ~ dnorm(0, .5)
t0 ~ dnorm(0, .5)
phi <- exp(t0)
}" 

d = data.frame(p= sample(c(.1,.2,.3,.4),100, replace= TRUE),
               id = seq(1,100,1))
id = d$id
y = d$p
model <- jags.model(textConnection(jags_str), 
                    data = list(y=y,id=id,len_id=length(id))
)
update(model, 10000, progress.bar="none"); # Burnin for 10000 samples
samp <- coda.samples(model, 
                     variable.names=c("a","b","phi"), 
                     n.iter=20000, progress.bar="none")

summary(samp)
plot(samp)



```


<https://people.ucsc.edu/~abrsvn/general_correlated_ranefs_bayes_jags.r> Mixed effects models in BUGS (JAGS)    
```{r}
# Contents:
# -- linear mixed-effects models---random intercepts only, independent random intercepts and slopes, correlated random intercepts and slopes (simulated data, R analysis, JAGS analysis)
# -- modeling 2 correlated random effects with a scaled Inverse Wishart instead (simulated data, R analysis, JAGS analysis)
# -- general correlated random effects with a scaled Inverse Wishart (simulated data, R analysis, JAGS analysis)
# -- the material in this presentation is based on Kery (2010), Gelman & Hill (2007), and notes here http://www.unc.edu/courses/2010fall/ecol/563/001/docs/lectures/lecture28.htm and here http://people.tamu.edu/~daveamp/Bugs.ppt


# Linear mixed-effects models

# Mixed-effects or mixed models contain factors, or more generally covariates, with both fixed and random effects.
# -- we constrain the values for at least one set of effects (intercepts and/or slopes) to come from a normal distribution
# -- this is what the random-effects assumption means (usually; in general, the random effects can come from any distribution)

# There are at least three sets of assumptions that one may make about the random effects for the intercept and/or the slope of regression lines that are fitted to grouped data:
### 1. only intercepts are random, but slopes are identical for all groups (e.g., subjects, items etc.)
### 2. both intercepts and slopes are random, but they are independent
### 3. both intercepts and slopes are random and there is a correlation between them

### (an additional case, where slopes are random and intercepts are fixed, is not a sensible model in most circumstances)


# Model No. 1 is often called a random-intercepts model.
# Both models No. 2 and 3 are called random-coefficients models.
# Model No. 3 is the default in R's function lmer() in package lme4 when fitting a random-coefficients model.


# The plan:
# -- [Model 1 and Model 2] first, we generate a random-coefficients dataset under model No. 2, where both intercepts and slopes are uncorrelated random effects; we then fit both a random-intercepts (No. 1) and a random-coefficients model without correlation (No. 2) to this dataset
# -- [Model 3] next, we generate a second data set that includes a correlation between random intercepts and random slopes and adopt the random-coefficients model with correlation between intercepts and slopes (No. 3) to analyze it
# -- [Model 3, ctd.] we estimate this model with JAGS in 2 different ways: (i) by separately modeling the sd.s of the 2 random-effect distributions (the intercepts and the slopes) and the correlation between the 2 distributions; (ii) by modeling all three parameters simultaneously with a scaled inverse Wishart prior; this second way has much better mixing and it also generalizes immediately to more complex random effects structures involving 3 or more correlated random effects
# -- [Model 4] finally, we generate a third data set that includes 3 correlated random effects (intercepts and 2 distinct slope vectors) and show how the random-coefficients model with correlation between intercepts and slopes generalizes to account for this kind of random effect structures; we only estimate this with a scaled inverse Wishart prior


# A close examination of how such a dataset can be assembled (i.e., simulated) will help us better understand how analogous datasets are broken down (i.e., analyzed) using mixed models.
# -- as Kery (2010) puts it: very few strategies can be more effective to understand this type of mixed model than the combination of simulating data sets and describing the models fitted in BUGS syntax


### Both intercepts and slopes are uncorrelated random effects
# Data generation

# The factor for subjects:
n_subjects <- 56 # Number of subjects
n_sample <- 10 # Number of observations for each subject
(n_obs <- n_subjects*n_sample) # Total number of data points
(subjects <- gl(n=n_subjects, k=n_sample)) # Indicator for subjects, generate factor levels specifying pattern of levels

# Continuous predictor x
original_x <- runif(n_obs, 45, 70)
summary(original_x)

# We standardize it (always good to do with continuous predictors when using JAGS/BUGS)
(mean_orig_x <- mean(original_x))
(sd_orig_x <- sd(original_x))
x <- (original_x-mean_orig_x)/sd_orig_x
round(x, 2)
summary(x)

hist(x, col="lightsteelblue1", border="white", breaks=10, freq=FALSE)
lines(density(x), col="lightsteelblue3", lwd=2)


# This is the model matrix for the means parametrization of the interaction model between the subjects and the continuous covariate x:
Xmat <- model.matrix(~subjects*x-1-x)
dim(Xmat)

# Q: where do these dimensions come from?
head(Xmat)

# -- there are 560 observations (rows) and 112 regression terms / variables (columns)
dimnames(Xmat)[[1]]
dimnames(Xmat)[[2]]

# -- there are 56 terms for subjects, the coefficients of which will provide the subject-specific intercepts (i.e., the intercept random effects, or intercept effects for short)
# -- there are 56 terms for interactions between each subject and the continuous covariate x, the coefficients of which will provide the subject-specific slopes (i.e., the slope random effects, or slope effects for short)

round(Xmat[1, ], 2) 		# Print the top row for each column, columns are random intercept for each subject and then random slope for each
Xmat[, 1]               # Print all rows for column 1 (group 1), just has 1's in the first 10, 10 observations for each
round(Xmat[, 57], 2)    # Print all rows for column 57 (group 1:x), just values for continuous predictor x


# Parameters for the distributions of the random coefficients / random effects (note that the intercepts and slopes comes from two independent Gaussian distributions):

intercept_mean <- 230 # mu_alpha
intercept_sd <- 20 # sigma_alpha

slope_mean <- 60 # mu_beta
slope_sd <- 30 # sigma_beta


# Generate the random coefficients:
intercept_effects <- rnorm(n=n_subjects, mean=intercept_mean, sd=intercept_sd)
slope_effects <- rnorm(n=n_subjects, mean=slope_mean, sd=slope_sd)

par(mfrow=c(1, 2))
hist(intercept_effects, col="lightsteelblue1", border="white", breaks=10, freq=FALSE)
lines(density(intercept_effects), col="lightsteelblue3", lwd=2)
hist(slope_effects, col="lightsteelblue1", border="white", breaks=10, freq=FALSE)
lines(density(slope_effects), col="lightsteelblue3", lwd=2)
par(mfrow=c(1, 1))

all_effects <- c(intercept_effects, slope_effects) # Put them all together
round(all_effects, 2)

# -- thus, we have two stochastic components in our model IN ADDITION TO the usual stochastic component for the individual-level responses, to which we now turn


# Generating the continuous response variable:

# -- the deterministic part
lin_pred <- Xmat %*% all_effects # Value of lin_predictor; all_effects are the beta_0's for intercepts, beta_1 for slopes
str(lin_pred)

# -- the stochastic part
sigma_res <- 30
normal_error <- rnorm(n=n_obs, mean=0, sd=sigma_res) # residuals
str(normal_error)

# -- put the two together
y <- lin_pred+normal_error
str(y)
# or, alternatively
y <- rnorm(n=n_obs, mean=lin_pred, sd=sigma_res)
str(y)

# We take a look at the response variable
hist(y, col="lightsteelblue1", border="white", breaks=30, freq=FALSE)
lines(density(y), col="lightsteelblue3", lwd=2)
summary(y)

library("lattice")
xyplot(y~x|subjects)


# Analysis under a random-intercepts model

# REML analysis using R
library("lme4")
lme_fit1 <- lmer(y~x+(1|subjects))
print(lme_fit1, cor=FALSE)

fixef(lme_fit1)
ranef(lme_fit1)
coef(lme_fit1)

# Compare with true values:
data.frame(intercept_mean=intercept_mean, slope_mean=slope_mean, intercept_sd=intercept_sd, slope_sd=slope_sd, sigma_res=sigma_res)
print(lme_fit1, cor=FALSE)


# Bayesian analysis using JAGS

# Write model
cat("model {
  # Priors
  mu_int~dnorm(0, 0.0001) # Mean hyperparameter for random intercepts
  sigma_int~dunif(0, 100) # SD hyperparameter for random intercepts
  tau_int <- 1/(sigma_int*sigma_int)
    for (i in 1:n_subj) {
        alpha[i]~dnorm(mu_int, tau_int) # Random intercepts
    }
  beta~dnorm(0, 0.0001) # Common slope
  sigma_res~dunif(0, 100) # Residual standard deviation
  tau_res <- 1/(sigma_res*sigma_res)
  
  # Likelihood
  for (i in 1:n_obs) {
      mu[i] <- alpha[subjects[i]]+beta*x[i] # Expectation, random intercept, just the beta
      y[i]~dnorm(mu[i], tau_res) # The actual (random) responses
  }
}", fill=TRUE, file="lme_model1.txt")  ## will write to getwd(); ??fill??

# Bundle data
jags_data <- list(y=as.numeric(y), subjects=as.numeric(subjects), x=as.numeric(x), n_subj=max(as.numeric(subjects)), n_obs=as.numeric(n_obs))
# use as.numeric across the board for the data passed to JAGS; it might work w/o it, but this is often needed for other BUGS packages

# Inits function
inits <- function() {
    list(alpha=rnorm(n_subjects, 0, 2), beta=rnorm(1, 1, 1), mu_int=rnorm(1, 0, 1), sigma_int=rlnorm(1), sigma_res=rlnorm(1))
}

# Parameters to estimate
params <- c("alpha", "beta", "mu_int", "sigma_int", "sigma_res")

# MCMC settings
ni <- 11000; nb <- 1000; nt <- 20; nc <- 3

# Start Gibbs sampling
library("R2jags")
outj <- jags(jags_data, inits=inits, parameters.to.save=params, model.file="lme_model1.txt", n.thin=nt, n.chains=nc, n.burnin=nb, n.iter=ni)

traceplot(outj)

# Inspect results
print(outj, dig=3)

out <- outj$BUGSoutput

# Compare with MLEs and true values
data.frame(intercept_mean=intercept_mean, slope_mean=slope_mean, intercept_sd=intercept_sd, slope_sd=slope_sd, sigma_res=sigma_res)
print(out$mean, dig=4)
print(lme_fit1, cor=FALSE)

# The same model can be parametrized by making the mean of the intercept ranefs a fixed effect and modeling the subject ranefs as coming from a normal distribution centered at 0. This is in fact how the lmer function models ranefs.

# alternative model
cat("model {
# Priors
sigma_int~dunif(0, 100) # SD hyperparameter for random intercepts
tau_int <- 1/(sigma_int*sigma_int)
for (i in 1:n_subj) {
    alpha[i]~dnorm(0, tau_int) # Random by-subject deflections to the intercept
}
mu_int~dnorm(0, 0.0001) # The mean intercept
beta~dnorm(0, 0.0001) # Common slope
sigma_res~dunif(0, 100) # Residual standard deviation
tau_res <- 1/(sigma_res*sigma_res)
# Likelihood
for (i in 1:n_obs) {
    mu[i] <- mu_int + alpha[subjects[i]] + beta*x[i] # Expectation
    y[i]~dnorm(mu[i], tau_res) # The actual (random) responses
}
}", fill=TRUE, file="lme_model1_1.txt")

# Bundle data
jags_data <- list(y=as.numeric(y), subjects=as.numeric(subjects), x=as.numeric(x), n_subj=max(as.numeric(subjects)), n_obs=as.numeric(n_obs))
# use as.numeric across the board for the data passed to JAGS; it might work w/o it, but this is often needed for other BUGS packages

# Inits function
inits <- function() {
    list(alpha=rnorm(n_subjects, 0, 2), beta=rnorm(1, 1, 1), mu_int=rnorm(1, 0, 1), sigma_int=rlnorm(1), sigma_res=rlnorm(1))
}

# Parameters to estimate
params <- c("alpha", "beta", "mu_int", "sigma_int", "sigma_res")

# MCMC settings
ni <- 11000; nb <- 1000; nt <- 20; nc <- 3

# Start Gibbs sampling
library("R2jags")
outj <- jags(jags_data, inits=inits, parameters.to.save=params, model.file="lme_model1_1.txt", n.thin=nt, n.chains=nc, n.burnin=nb, n.iter=ni)

traceplot(outj)

# Inspect results
print(outj, dig=3)

out <- outj$BUGSoutput

# Compare with MLEs and true values
data.frame(intercept_mean=intercept_mean, slope_mean=slope_mean, intercept_sd=intercept_sd, slope_sd=slope_sd, sigma_res=sigma_res)
print(out$mean, dig=4)
print(lme_fit1, cor=FALSE)


# Analysis under a random-coefficients model without correlation between intercept and slope

# REML analysis using R
library("lme4")
lme_fit2 <- lmer(y~x+(1|subjects)+(0+x|subjects))
print(lme_fit2, cor=F)
fixef(lme_fit2)
ranef(lme_fit2)
coef(lme_fit2)


# Compare with true values:
data.frame(intercept_mean=intercept_mean, slope_mean=slope_mean, intercept_sd=intercept_sd, slope_sd=slope_sd, sigma_res=sigma_res)
print(lme_fit2, cor=FALSE)


# Bayesian analysis using JAGS

# Define model
cat("model {
# Priors
mu_int~dnorm(0, 0.001) # Mean hyperparameter for random intercepts
sigma_int~dunif(0, 100) # SD hyperparameter for random intercepts
tau_int <- 1/(sigma_int*sigma_int)
mu_slope~dnorm(0, 0.001) # Mean hyperparameter for random slopes
sigma_slope~dunif(0, 100) # SD hyperparameter for slopes
tau_slope <- 1/(sigma_slope*sigma_slope)
for (i in 1:n_subj) {
    alpha[i]~dnorm(mu_int, tau_int) # Random intercepts
    beta[i]~dnorm(mu_slope, tau_slope) # Random slopes
}
sigma_res~dunif(0, 100) # Residual standard deviation
tau_res <- 1/(sigma_res*sigma_res) # Residual precision
# Likelihood
for (i in 1:n_obs) {
    mu[i] <- alpha[subjects[i]]+beta[subjects[i]]*x[i]
    y[i]~dnorm(mu[i], tau_res)
}
}", fill=TRUE, file="lme_model2.txt")

# Bundle data
jags_data <- list(y=as.numeric(y), subjects=as.numeric(subjects), x=as.numeric(x), n_subj=max(as.numeric(subjects)), n_obs=as.numeric(n_obs))

# Inits function
inits <- function() {
    list(alpha=rnorm(n_subjects, 0, 2), beta=rnorm(n_subjects, 10, 2), mu_int=rnorm(1, 0, 1), sigma_int=rlnorm(1), mu_slope=rnorm(1, 0, 1), sigma_slope=rlnorm(1), sigma_res=rlnorm(1))
}

# Parameters to estimate
params <- c("alpha", "beta", "mu_int", "sigma_int", "mu_slope", "sigma_slope", "sigma_res")

# MCMC settings
ni <- 11000; nb <- 1000; nt <- 10; nc <- 3

# Start Gibbs sampling
library("R2jags")
outj <- jags(jags_data, inits=inits, parameters.to.save=params, model.file="lme_model2.txt", n.thin=nt, n.chains=nc, n.burnin=nb, n.iter=ni)

traceplot(outj)

# -- you can see that the chains are mixing much better when we use the appropriate model for the data
# -- see Gelman's `folk theorem of statistical computing' (http://andrewgelman.com/2008/05/13/the_folk_theore/):
# when you have computational problems, often there's a problem with your model

print(outj, dig=3)

out <- outj$BUGSoutput

# Compare with MLEs and true values
data.frame(intercept_mean=intercept_mean, slope_mean=slope_mean, intercept_sd=intercept_sd, slope_sd=slope_sd, sigma_res=sigma_res)
print(out$mean, dig=4)
print(lme_fit2, cor=FALSE)

# Using simulated data and successfully recovering the input values makes us fairly confident that the JAGS analysis has been correctly specified.

# This is very helpful for more complex models b/c it's easy to make mistakes:
# -- a good way to check the JAGS analysis for a custom model that is needed for a particular phenomenon is to simulate the data and run the JAGS model on that data


# [Model 3] The random-coefficients model with correlation between intercept and slope

# Data generation

n_subjects <- 56
n_sample <- 10
(n_obs <- n_subjects*n_sample)
(subjects <- gl(n=n_subjects, k=n_sample))

# Standardized continuous covariate:
original_x <- runif(n_obs, 45, 70)
(mean_orig_x <- mean(original_x))
(sd_orig_x <- sd(original_x))
x <- (original_x-mean_orig_x)/sd_orig_x

hist(x, col="lightsteelblue1", border="white", breaks=20, freq=FALSE)
lines(density(x), col="lightsteelblue3", lwd=2)

# Design matrix:
Xmat <- model.matrix(~subjects*x-1-x)

# -- there are 560 observations (rows) and 112 regression terms / variables (columns), just as before
dimnames(Xmat)[[1]]
dimnames(Xmat)[[2]]

round(Xmat[1, ], 2) # Print the top row for each column

# Generate the correlated random effects for intercept and slope:


# Assembling the parameters for the multivariate normal distribution

intercept_mean <- 230 # Values for five hyperparameters
intercept_sd <- 20
slope_mean <- 60
slope_sd <- 30
intercept_slope_covariance <- 10
intercept_slope_correlation <- intercept_slope_covariance/(intercept_sd*slope_sd)

(mu_vector <- c(intercept_mean, slope_mean))
(var_covar_matrix <- matrix(c(intercept_sd^2, intercept_slope_covariance, intercept_slope_covariance, slope_sd^2), 2, 2))

# Generating the correlated random effects for intercepts and slopes:

library("MASS") # Load MASS to sample from a multivariate normal
effects <- mvrnorm(n=n_subjects, mu=mu_vector, Sigma=var_covar_matrix)
round(effects, 2)

par(mfrow=c(1, 2))
hist(effects[, 1], col="lightsteelblue1", border="white", breaks=10, freq=FALSE)
lines(density(effects[, 1]), col="lightsteelblue3", lwd=2)

hist(effects[, 2], col="lightsteelblue1", border="white", breaks=10, freq=FALSE)
lines(density(effects[, 2]), col="lightsteelblue3", lwd=2)
par(mfrow=c(1, 1))

# Plotting the bivariate distribution:
effects_kde <- kde2d(effects[, 1], effects[, 2], n=50) # kernel density estimate
par(mfrow=c(1, 3))
contour(effects_kde)
image(effects_kde)
persp(effects_kde, phi=45, theta=30)

# even better:
par(mfrow=c(1, 2))
image(effects_kde); contour(effects_kde, add=T)
persp(effects_kde, phi=45, theta=-30, shade=.1, border=NULL, col="lightsteelblue1", ticktype="detailed", xlab="", ylab="", zlab="")
par(mfrow=c(1, 1))

apply(effects, 2, mean)
apply(effects, 2, sd)
apply(effects, 2, var)
cov(effects[, 1], effects[, 2])
var(effects)

# Population parameters
data.frame(intercept_mean=intercept_mean, slope_mean=slope_mean, intercept_sd=intercept_sd, slope_sd=slope_sd, intercept_slope_covariance=intercept_slope_covariance, sigma_res=sigma_res)

# Sampling error for intercept-slope covariance (200 samples of 50, 500, 5000 and 50000 group/random effects each) -- just to illustrate how difficult it is to estimate measures of association, even from fairly large data sets with 5000 observations:

par(mfrow=c(1, 4))
cov_temp1 <- numeric()
for (i in 1:200) {
    temp1 <- mvrnorm(50, mu=mu_vector, Sigma=var_covar_matrix)
    cov_temp1[i] <- var(temp1)[1, 2]
}
hist(cov_temp1, col="lightsteelblue1", border="white", freq=FALSE, main="n_obs=50")
lines(density(cov_temp1), col="lightsteelblue3", lwd=2)

cov_temp2 <- numeric()
for (i in 1:200) {
    temp2 <- mvrnorm(500, mu=mu_vector, Sigma=var_covar_matrix)
    cov_temp2[i] <- var(temp2)[1, 2]
}
hist(cov_temp2, col="lightsteelblue1", border="white", freq=FALSE, main="n_obs=500")
lines(density(cov_temp2), col="lightsteelblue3", lwd=2)

cov_temp3 <- numeric()
for (i in 1:200) {
    temp3 <- mvrnorm(5000, mu=mu_vector, Sigma=var_covar_matrix)
    cov_temp3[i] <- var(temp3)[1, 2]
}
hist(cov_temp3, col="lightsteelblue1", border="white", freq=FALSE, main="n_obs=5000")
lines(density(cov_temp3), col="lightsteelblue3", lwd=2)

cov_temp4 <- numeric()
for (i in 1:200) {
    temp4 <- mvrnorm(50000, mu=mu_vector, Sigma=var_covar_matrix)
    cov_temp4[i] <- var(temp4)[1, 2]
}
hist(cov_temp4, col="lightsteelblue1", border="white", freq=FALSE, main="n_obs=50000")
lines(density(cov_temp4), col="lightsteelblue3", lwd=2)
par(mfrow=c(1, 1))


intercept_effects <- effects[, 1]
round(intercept_effects, 2)
slope_effects <- effects[, 2]
round(slope_effects, 2)
all_effects <- c(intercept_effects, slope_effects) # Put them all together
round(all_effects, 2)


# Generate the response variable:
# -- the deterministic part
lin_pred <- Xmat %*% all_effects
round(as.vector(lin_pred), 2)

# -- the stochastic part
sigma_res <- 30
(normal_error <- rnorm(n=n_obs, mean=0, sd=sigma_res))	# residuals

# -- add them together
y <- lin_pred+normal_error
# or, in one go:
y <- rnorm(n=n_obs, mean=lin_pred, sd=sigma_res)

hist(y, col="lightsteelblue1", border="white", breaks=15, freq=FALSE)
lines(density(y), col="lightsteelblue3", lwd=2)

library("lattice")
xyplot(y~x|subjects, pch=20)


# REML analysis using R
library("lme4")
lme_fit3 <- lmer(y~x+(x|subjects))
print(lme_fit3, cor=FALSE)

# Compare with the true values:
data.frame(intercept_mean=intercept_mean, slope_mean=slope_mean, intercept_sd=intercept_sd, slope_sd=slope_sd, intercept_slope_correlation=intercept_slope_covariance/(intercept_sd*slope_sd), sigma_res=sigma_res)



# Bayesian analysis using JAGS

# This is one way in which we can specify a Bayesian analysis of the random-coefficients model with correlation.
# -- it is more intuitive but does not generalize well to more than 2 correlated random effects.

# We will introduce a different and more general way to allow for correlation among two or more sets of random effects in a model after this.

# Define model
cat("model {
# Priors
mu_int~dnorm(0, 0.0001) # mean for random intercepts
mu_slope~dnorm(0, 0.0001) # mean for random slopes
sigma_int~dunif(0, 100) # SD of intercepts
sigma_slope~dunif(0, 100) # SD of slopes
rho~dunif(-1, 1) # correlation between intercepts and slopes
Sigma_B[1, 1] <- pow(sigma_int, 2) # We start assembling the var-covar matrix for the random effects
Sigma_B[2, 2] <- pow(sigma_slope, 2)
Sigma_B[1, 2] <- rho*sigma_int*sigma_slope
Sigma_B[2, 1] <- Sigma_B[1, 2]
covariance <- Sigma_B[1, 2]
Tau_B[1:2, 1:2] <- inverse(Sigma_B[,])
for (i in 1:n_subj) {
    B_hat[i, 1] <- mu_int
    B_hat[i, 2] <- mu_slope
    B[i, 1:2]~dmnorm(B_hat[i, ], Tau_B[,]) # the pairs of correlated random effects
    alpha[i] <- B[i, 1] # random intercept
    beta[i] <- B[i, 2] # random slope
}
sigma_res~dunif(0, 100) # Residual standard deviation
tau_res <- 1/(sigma_res*sigma_res)
# Likelihood
for (i in 1:n_obs) {
    mu[i] <- alpha[subjects[i]]+beta[subjects[i]]*x[i]
    y[i]~dnorm(mu[i], tau_res)
}
}", fill=TRUE, file="lme_model3.txt")

# Bundle data
jags_data <- list(y=as.numeric(y), subjects=as.numeric(subjects), x=as.numeric(x), n_subj=max(as.numeric(subjects)), n_obs=as.numeric(n_obs))

# Inits function
inits <- function() {
    list(mu_int=rnorm(1, 0, 1), sigma_int=rlnorm(1), mu_slope=rnorm(1, 0, 1), sigma_slope=rlnorm(1), rho=runif(1, -1, 1), sigma_res=rlnorm(1))
}

# Parameters to estimate
params <- c("alpha", "beta", "mu_int", "sigma_int", "mu_slope", "sigma_slope", "rho", "covariance", "sigma_res")

# MCMC settings
ni <- 3200; nb <- 200; nt <- 6; nc <- 3 # more than this probably needed for a good approx. of the posterior distribution

# Start Gibbs sampler
library("R2jags")
outj <- jags(jags_data, inits=inits, parameters.to.save=params, model.file="lme_model3.txt", n.thin=nt, n.chains=nc, n.burnin=nb, n.iter=ni)
traceplot(outj)

# -- this type of model does not converge very fast in addition to the fact that it does not generalize very well; we increase the number of iterations

ni <- 25000; nb <- 5000; nt <- 40; nc <- 3
outj <- jags(jags_data, inits=inits, parameters.to.save=params, model.file="lme_model3.txt", n.thin=nt, n.chains=nc, n.burnin=nb, n.iter=ni)
traceplot(outj)

print(outj, dig=3)

out <- outj$BUGSoutput

# Compare with MLEs and true values
data.frame(intercept_mean=intercept_mean, slope_mean=slope_mean, intercept_sd=intercept_sd, slope_sd=slope_sd, intercept_slope_correlation=intercept_slope_covariance/(intercept_sd*slope_sd), sigma_res=sigma_res)
print(out$mean, dig=4)
print(lme_fit3, cor=FALSE)

# Note the very large SD for the posterior distribution of the covariance (relative to the mean):
print(out$mean$covariance, dig=2)
print(out$sd$covariance, dig=2)

print(out$mean$rho, dig=2)
print(out$sd$rho, dig=2)

# -- R does not even provide an SE for the covariance estimator (equivalently, for the correlation of random effects)
# -- covariances are even harder to reliably estimate than variances, which are harder than mean estimators (it's easy to estimate measures of center / location, harder to estimate measures of dispersion and even harder to estimate measures of association)


# MODELING CORRELATED RANEFS WITH A SCALED INVERSE WISHART: the 2 correlated ranefs case first.

# -- we will now introduce an alternative way of placing priors over correlated random effects that both converges faster and generalizes to structures with more than 2 correlated random effects.
# -- see the relevant chapter of Gelman & Hill (2007) for more introductory discussion and references

# Bayesian analysis using a scaled Inverse Wishart

# Define model
cat("model {

# Set up the means for the multivariate ranef distribution
for (i in 1:2) {
    xi[i]~dunif(0, 100) # scaling for the multivariate ranef distribution (for means, sds, and the ranefs themselves)
    mu_raw[i]~dnorm(0, .0001) # unscaled means for the multivariate ranef distribution
    mu[i] <- xi[i]*mu_raw[i] # scaled means for the multivariate ranef distribution
}
mu_int <- mu[1] # mean for random intercepts
mu_slope <- mu[2] # mean for random slopes

# Set up the var-covar matrix for the multivariate ranef distribution
Tau_B_raw[1:2, 1:2] ~ dwish(W[,], 3) # W is the identity matrix, provided as data; we have 3 dofs, i.e., 2 ranefs + 1, to ensure a uniform (-1, 1) prior for the correlation between ranefs
Sigma_B_raw[1:2, 1:2] <- inverse(Tau_B_raw[,])
for (i in 1:2) {
    sigma[i] <- xi[i]*sqrt(Sigma_B_raw[i, i])
}
sigma_int <- sigma[1] # SD of intercepts
sigma_slope <- sigma[2] # SD of slopes
for (i in 1:2) { for (j in 1:2) {
    rho[i, j] <- Sigma_B_raw[i, j]/sqrt(Sigma_B_raw[i, i]*Sigma_B_raw[j, j])
} }
rho_int_slope <- rho[1, 2]
covariance <- rho_int_slope*sigma_int*sigma_slope

# The multivariate ranef distribution, i.e., modeling the correlated ranefs
for (j in 1:n_subj) {
	B_raw_hat[j, 1] <- mu_raw[1]
	B_raw_hat[j, 2] <- mu_raw[2]
	B_raw[j, 1:2] ~ dmnorm(B_raw_hat[j, ], Tau_B_raw[, ]) # the pairs of unscaled (raw) correlated random effects
	alpha[j] <- xi[1]*B_raw[j, 1] # random intercept
    beta[j] <- xi[2]*B_raw[j, 2] # random slope
}

# Model the resid. sd independently
sigma_res~dunif(0, 100) # Residual standard deviation
tau_res <- 1/(sigma_res*sigma_res)

# Likelihood
for (i in 1:n_obs) {
    mu_obs[i] <- alpha[subjects[i]]+beta[subjects[i]]*x[i]
    y[i]~dnorm(mu_obs[i], tau_res)
}

# Sampling from the prior: given that we do not place hyperpriors directly on the means, sds and correlation(s) of the multivariate ranef distribution, we want to sample from the prior to make sure we didn't accidentally make it more informed than we wanted (and we want it very vague)
for (i in 1:2) {
    xi_prior[i]~dunif(0, 100)
    mu_raw_prior[i]~dnorm(0, .0001)
    mu_prior[i] <- xi_prior[i]*mu_raw_prior[i]
}
mu_int_prior <- mu_prior[1]
mu_slope_prior <- mu_prior[2]
Tau_B_raw_prior[1:2, 1:2] ~ dwish(W[,], 3)
Sigma_B_raw_prior[1:2, 1:2] <- inverse(Tau_B_raw_prior[,])
for (i in 1:2) {
    sigma_prior[i] <- xi_prior[i]*sqrt(Sigma_B_raw_prior[i, i])
}
sigma_int_prior <- sigma_prior[1]
sigma_slope_prior <- sigma_prior[2]
for (i in 1:2) { for (j in 1:2) {
    rho_prior[i, j] <- Sigma_B_raw_prior[i, j]/sqrt(Sigma_B_raw_prior[i, i]*Sigma_B_raw_prior[j, j])
} }
rho_int_slope_prior <- rho_prior[1, 2]
}", fill=TRUE, file="lme_model4.txt")

# Bundle data
jags_data <- list(y=as.numeric(y), subjects=as.numeric(subjects), x=as.numeric(x), n_subj=max(as.numeric(subjects)), n_obs=as.numeric(n_obs), W=diag(2))

#install.packages("bayesm")
library("bayesm")
var_vec <- apply(coef(lme_fit3)$subjects, 2, var)

# Inits function
inits <- function() {
    list(xi=rlnorm(2), mu_raw=rnorm(2), Tau_B_raw=rwishart(3, diag(2)*var_vec)$W, sigma_res=rlnorm(1), xi_prior=rlnorm(2), mu_raw_prior=rnorm(2), Tau_B_raw_prior=rwishart(3, diag(2)*var_vec)$W)
}

# Parameters to estimate
params <- c("mu", "mu_int", "mu_slope", "sigma", "sigma_int", "sigma_slope", "rho", "rho_int_slope", "covariance", "alpha", "beta", "sigma_res", "mu_int_prior", "mu_slope_prior", "sigma_int_prior", "sigma_slope_prior", "rho_int_slope_prior")

# MCMC settings
ni <- 7000; nb <- 1000; nt <- 6; nc <- 3 # more than this probably needed for a good approx. of the posterior distribution

# Start Gibbs sampler
library("R2jags")
outj <- jags(jags_data, inits=inits, parameters.to.save=params, model.file="lme_model4.txt", n.thin=nt, n.chains=nc, n.burnin=nb, n.iter=ni)

traceplot(outj)

print(outj, dig=3)

out <- outj$BUGSoutput

# Compare with MLEs and true values
data.frame(intercept_mean=intercept_mean, slope_mean=slope_mean, intercept_sd=intercept_sd, slope_sd=slope_sd, intercept_slope_correlation=intercept_slope_covariance/(intercept_sd*slope_sd), sigma_res=sigma_res)
print(out$mean, dig=4)
print(lme_fit3, cor=FALSE)

# Once again, note the very large SD for the posterior distribution of the covariance (relative to the mean):
print(out$mean$covariance, dig=2)
print(out$sd$covariance, dig=2)

print(out$mean$rho, dig=2)
print(out$sd$rho, dig=2)

# Finally, we compare the prior and posterior distributions for the (derived) ranef distribution parameters:

par(mfrow=c(3, 4))
hist(out$sims.list$mu_int_prior, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="mu_int_prior")
lines(density(out$sims.list$mu_int_prior), col="lightsteelblue3", lwd=2)
hist(out$sims.list$mu_int, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="mu_int")
lines(density(out$sims.list$mu_int), col="lightsteelblue3", lwd=2)

hist(out$sims.list$mu_slope_prior, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="mu_slope_prior")
lines(density(out$sims.list$mu_slope_prior), col="lightsteelblue3", lwd=2)
hist(out$sims.list$mu_slope, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="mu_slope")
lines(density(out$sims.list$mu_slope), col="lightsteelblue3", lwd=2)

hist(out$sims.list$sigma_int_prior, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="sigma_int_prior")
lines(density(out$sims.list$sigma_int_prior), col="lightsteelblue3", lwd=2)
hist(out$sims.list$sigma_int, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="sigma_int")
lines(density(out$sims.list$sigma_int), col="lightsteelblue3", lwd=2)

hist(out$sims.list$sigma_slope_prior, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="sigma_slope_prior")
lines(density(out$sims.list$sigma_slope_prior), col="lightsteelblue3", lwd=2)
hist(out$sims.list$sigma_slope, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="sigma_slope")
lines(density(out$sims.list$sigma_slope), col="lightsteelblue3", lwd=2)

hist(out$sims.list$rho_int_slope_prior, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="rho_int_slope_prior")
lines(density(out$sims.list$rho_int_slope_prior), col="lightsteelblue3", lwd=2)
hist(out$sims.list$rho_int_slope, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="rho_int_slope")
lines(density(out$sims.list$rho_int_slope), col="lightsteelblue3", lwd=2)
par(mfrow=c(1, 1))


# [Model 4] Finally, we simulate data from a model with 3 correlated ranefs and then provide the R and JAGS analyses of this data

# Data generation

n_subjects <- 56
n_sample <- 10
(n_obs <- n_subjects*n_sample)
(subjects <- gl(n=n_subjects, k=n_sample))

# Two standardized continuous covariates:
original_x1 <- runif(n_obs, 45, 70)
(mean_orig_x1 <- mean(original_x1))
(sd_orig_x1 <- sd(original_x1))
x1 <- (original_x1-mean_orig_x1)/sd_orig_x1

original_x2 <- rgamma(n_obs, 10, 1)
(mean_orig_x2 <- mean(original_x2))
(sd_orig_x2 <- sd(original_x2))
x2 <- (original_x2-mean_orig_x2)/sd_orig_x2

par(mfrow=c(1, 2))
hist(x1, col="lightsteelblue1", border="white", breaks=15, freq=FALSE)
lines(density(x1), col="lightsteelblue3", lwd=2)

hist(x2, col="lightsteelblue1", border="white", breaks=15, freq=FALSE)
lines(density(x2), col="lightsteelblue3", lwd=2)
par(mfrow=c(1, 1))

# Design matrix:
Xmat <- model.matrix(~subjects*x1+subjects*x2-1-x1-x2)

# -- there are 560 observations (rows) and 112 regression terms / variables (columns), just as before
dimnames(Xmat)[[1]]
dimnames(Xmat)[[2]]

round(Xmat[1, ], 2) # Print the top row for each column

# Generate the correlated random effects for intercept and slope:


# Assembling the parameters for the multivariate normal distribution

intercept_mean <- 230
intercept_sd <- 20
slope1_mean <- 60
slope1_sd <- 30
slope2_mean <- 40
slope2_sd <- 25
intercept_slope1_covariance <- 10
(intercept_slope1_correlation <- intercept_slope1_covariance/(intercept_sd*slope1_sd))
intercept_slope2_covariance <- 300
(intercept_slope2_correlation <- intercept_slope2_covariance/(intercept_sd*slope2_sd))
slope1_slope2_covariance <- -200
(slope1_slope2_correlation <- slope1_slope2_covariance/(slope1_sd*slope2_sd))

(mu_vector <- c(intercept_mean, slope1_mean, slope2_mean))
(var_covar_matrix <- matrix(c(intercept_sd^2, intercept_slope1_covariance, intercept_slope2_covariance, intercept_slope1_covariance, slope1_sd^2, slope1_slope2_covariance, intercept_slope2_covariance, slope1_slope2_covariance, slope2_sd^2), 3, 3))
eigen(var_covar_matrix)$values

# Generating the correlated random effects for intercepts and slopes:

library("MASS")
effects <- mvrnorm(n=n_subjects, mu=mu_vector, Sigma=var_covar_matrix)
str(effects)
round(effects, 2)

par(mfrow=c(1, 3))
hist(effects[, 1], col="lightsteelblue1", border="white", breaks=10, freq=FALSE)
lines(density(effects[, 1]), col="lightsteelblue3", lwd=2)

hist(effects[, 2], col="lightsteelblue1", border="white", breaks=10, freq=FALSE)
lines(density(effects[, 2]), col="lightsteelblue3", lwd=2)

hist(effects[, 3], col="lightsteelblue1", border="white", breaks=10, freq=FALSE)
lines(density(effects[, 3]), col="lightsteelblue3", lwd=2)
par(mfrow=c(1, 1))

# Plotting the bivariate distributions:
effects_kde12 <- kde2d(effects[, 1], effects[, 2], n=50)
effects_kde13 <- kde2d(effects[, 1], effects[, 3], n=50)
effects_kde23 <- kde2d(effects[, 2], effects[, 3], n=50)

par(mfrow=c(3, 2))
image(effects_kde12); contour(effects_kde12, add=T)
persp(effects_kde12, phi=45, theta=-30, shade=.1, border=NULL, col="lightsteelblue1", ticktype="detailed", xlab="", ylab="", zlab="")
image(effects_kde13); contour(effects_kde13, add=T)
persp(effects_kde13, phi=45, theta=-30, shade=.1, border=NULL, col="lightsteelblue1", ticktype="detailed", xlab="", ylab="", zlab="")
image(effects_kde23); contour(effects_kde23, add=T)
persp(effects_kde23, phi=45, theta=-30, shade=.1, border=NULL, col="lightsteelblue1", ticktype="detailed", xlab="", ylab="", zlab="")
par(mfrow=c(1, 1))

apply(effects, 2, mean)
apply(effects, 2, sd)
apply(effects, 2, var)
cov(effects[, 1], effects[, 2])
cov(effects[, 1], effects[, 3])
cov(effects[, 2], effects[, 3])
var(effects)

sigma_res <- 30

# Population parameters
data.frame(intercept_mean=intercept_mean, slope1_mean=slope1_mean, slope2_mean=slope2_mean, intercept_sd=intercept_sd, slope1_sd=slope1_sd, slope2_sd=slope2_sd, intercept_slope1_covariance=intercept_slope1_covariance, intercept_slope2_covariance=intercept_slope2_covariance, slope1_slope2_covariance=slope1_slope2_covariance, sigma_res=sigma_res)

intercept_effects <- effects[, 1]
round(intercept_effects, 2)
slope1_effects <- effects[, 2]
round(slope1_effects, 2)
slope2_effects <- effects[, 3]
round(slope2_effects, 2)
all_effects <- c(intercept_effects, slope1_effects, slope2_effects)


# Generate the response variable:
# -- the deterministic part
lin_pred <- Xmat %*% all_effects
round(as.vector(lin_pred), 2)

# -- the stochastic part
sigma_res <- 30
(normal_error <- rnorm(n=n_obs, mean=0, sd=sigma_res))	# residuals

# -- add them together
y <- lin_pred+normal_error
# or, in one go:
y <- rnorm(n=n_obs, mean=lin_pred, sd=sigma_res)

hist(y, col="lightsteelblue1", border="white", breaks=15, freq=FALSE)
lines(density(y), col="lightsteelblue3", lwd=2)

library("lattice")
xyplot(y~x1+x2|subjects, pch=20)

data.frame(y, x1, x2, subjects)
# REML analysis using R
library("lme4")
lme_fit4 <- lmer(y~x1+x2+(x1+x2|subjects))
print(lme_fit4, cor=FALSE)

# Compare with the true values:
data.frame(intercept_mean=intercept_mean, slope1_mean=slope1_mean, slope2_mean=slope2_mean, intercept_sd=intercept_sd, slope1_sd=slope1_sd, slope2_sd=slope2_sd, intercept_slope1_covariance=intercept_slope1_covariance, intercept_slope2_covariance=intercept_slope2_covariance, slope1_slope2_covariance=slope1_slope2_covariance, sigma_res=sigma_res)

# Bayesian analysis using a scaled Inverse Wishart

# Define model
cat("model {

# Set up the means for the multivariate ranef distribution
for (i in 1:3) {
    xi[i]~dunif(0, 100)
    mu_raw[i]~dnorm(0, .0001)
    mu[i] <- xi[i]*mu_raw[i]
}
mu_int <- mu[1]
mu_slope1 <- mu[2]
mu_slope2 <- mu[3]

Tau_B_raw[1:3, 1:3] ~ dwish(W[,], 4)
Sigma_B_raw[1:3, 1:3] <- inverse(Tau_B_raw[,])
for (i in 1:3) {
    sigma[i] <- xi[i]*sqrt(Sigma_B_raw[i, i])
}
sigma_int <- sigma[1]
sigma_slope1 <- sigma[2]
sigma_slope2 <- sigma[3]
for (i in 1:3) { for (j in 1:3) {
    rho[i, j] <- Sigma_B_raw[i, j]/sqrt(Sigma_B_raw[i, i]*Sigma_B_raw[j, j])
} }
rho_int_slope1 <- rho[1, 2]
rho_int_slope2 <- rho[1, 3]
rho_slope1_slope2 <- rho[2, 3]

for (j in 1:n_subj) {
	B_raw_hat[j, 1] <- mu_raw[1]
	B_raw_hat[j, 2] <- mu_raw[2]
	B_raw_hat[j, 3] <- mu_raw[3]
	B_raw[j, 1:3] ~ dmnorm(B_raw_hat[j, ], Tau_B_raw[, ])
	alpha[j] <- xi[1]*B_raw[j, 1]
    beta1[j] <- xi[2]*B_raw[j, 2]
    beta2[j] <- xi[3]*B_raw[j, 3]
}

sigma_res~dunif(0, 100) # Residual standard deviation
tau_res <- 1/(sigma_res*sigma_res)

for (i in 1:n_obs) {
    mu_obs[i] <- alpha[subjects[i]]+beta1[subjects[i]]*x1[i]+beta2[subjects[i]]*x2[i]
    y[i]~dnorm(mu_obs[i], tau_res)
}

for (i in 1:3) {
    xi_prior[i]~dunif(0, 100)
    mu_raw_prior[i]~dnorm(0, .0001)
    mu_prior[i] <- xi_prior[i]*mu_raw_prior[i]
}
mu_int_prior <- mu_prior[1]
mu_slope1_prior <- mu_prior[2]
mu_slope2_prior <- mu_prior[3]
Tau_B_raw_prior[1:3, 1:3] ~ dwish(W[,], 4)
Sigma_B_raw_prior[1:3, 1:3] <- inverse(Tau_B_raw_prior[,])
for (i in 1:3) {
    sigma_prior[i] <- xi_prior[i]*sqrt(Sigma_B_raw_prior[i, i])
}
sigma_int_prior <- sigma_prior[1]
sigma_slope1_prior <- sigma_prior[2]
sigma_slope2_prior <- sigma_prior[3]
for (i in 1:3) { for (j in 1:3) {
    rho_prior[i, j] <- Sigma_B_raw_prior[i, j]/sqrt(Sigma_B_raw_prior[i, i]*Sigma_B_raw_prior[j, j])
} }
rho_int_slope1_prior <- rho_prior[1, 2]
rho_int_slope2_prior <- rho_prior[1, 3]
rho_slope1_slope2_prior <- rho_prior[2, 3]
}", fill=TRUE, file="lme_model5.txt")

# Bundle data
jags_data <- list(y=as.numeric(y), subjects=as.numeric(subjects), x1=as.numeric(x1), x2=as.numeric(x2), n_subj=max(as.numeric(subjects)), n_obs=as.numeric(n_obs), W=diag(3))

#install.packages("bayesm")
library("bayesm")
var_vec <- apply(coef(lme_fit4)$subjects, 2, var)

# Inits function
inits <- function() {
    list(xi=rlnorm(3), mu_raw=rnorm(3), Tau_B_raw=rwishart(4, diag(3)*var_vec)$W, sigma_res=rlnorm(1), xi_prior=rlnorm(3), mu_raw_prior=rnorm(3), Tau_B_raw_prior=rwishart(4, diag(3)*var_vec)$W)
}

# Parameters to estimate
params <- c("mu", "mu_int", "mu_slope1", "mu_slope2", "sigma", "sigma_int", "sigma_slope1", "sigma_slope2", "rho", "rho_int_slope1", "rho_int_slope2", "rho_slope1_slope2", "alpha", "beta1", "beta2", "sigma_res", "mu_int_prior", "mu_slope1_prior", "mu_slope2_prior", "sigma_int_prior", "sigma_slope1_prior", "sigma_slope2_prior", "rho_prior", "rho_int_slope1_prior", "rho_int_slope2_prior", "rho_slope1_slope2_prior")

# MCMC settings
ni <- 7000; nb <- 1000; nt <- 6; nc <- 3 # more than this probably needed for a good approx. of the posterior distribution

# Start Gibbs sampler
library("R2jags")
outj <- jags(jags_data, inits=inits, parameters.to.save=params, model.file="lme_model5.txt", n.thin=nt, n.chains=nc, n.burnin=nb, n.iter=ni)

traceplot(outj)

print(outj, dig=3)

out <- outj$BUGSoutput

# Compare with MLEs and true values
print(out$mean, dig=4)
data.frame(intercept_mean=intercept_mean, slope1_mean=slope1_mean, slope2_mean=slope2_mean, intercept_sd=intercept_sd, slope1_sd=slope1_sd, slope2_sd=slope2_sd, intercept_slope1_covariance=intercept_slope1_covariance, intercept_slope2_covariance=intercept_slope2_covariance, slope1_slope2_covariance=slope1_slope2_covariance, sigma_res=sigma_res)
print(lme_fit4, cor=FALSE)
data.frame(intercept_mean=intercept_mean, slope1_mean=slope1_mean, slope2_mean=slope2_mean, intercept_sd=intercept_sd, slope1_sd=slope1_sd, slope2_sd=slope2_sd, intercept_slope1_covariance=intercept_slope1_covariance, intercept_slope2_covariance=intercept_slope2_covariance, slope1_slope2_covariance=slope1_slope2_covariance, sigma_res=sigma_res)

# Note the large sd.s for the measures of association
print(out$mean$rho, dig=2)
print(out$sd$rho, dig=2)

# Finally, we compare the prior and posterior distributions for the (derived) ranef distribution parameters:

par(mfrow=c(3, 6))
hist(out$sims.list$mu_int_prior, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="mu_int_prior")
lines(density(out$sims.list$mu_int_prior), col="lightsteelblue3", lwd=2)
hist(out$sims.list$mu_int, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="mu_int")
lines(density(out$sims.list$mu_int), col="lightsteelblue3", lwd=2)

hist(out$sims.list$mu_slope1_prior, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="mu_slope1_prior")
lines(density(out$sims.list$mu_slope1_prior), col="lightsteelblue3", lwd=2)
hist(out$sims.list$mu_slope1, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="mu_slope1")
lines(density(out$sims.list$mu_slope1), col="lightsteelblue3", lwd=2)

hist(out$sims.list$mu_slope2_prior, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="mu_slope2_prior")
lines(density(out$sims.list$mu_slope2_prior), col="lightsteelblue3", lwd=2)
hist(out$sims.list$mu_slope2, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="mu_slope2")
lines(density(out$sims.list$mu_slope2), col="lightsteelblue3", lwd=2)

hist(out$sims.list$sigma_int_prior, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="sigma_int_prior")
lines(density(out$sims.list$sigma_int_prior), col="lightsteelblue3", lwd=2)
hist(out$sims.list$sigma_int, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="sigma_int")
lines(density(out$sims.list$sigma_int), col="lightsteelblue3", lwd=2)

hist(out$sims.list$sigma_slope1_prior, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="sigma_slope1_prior")
lines(density(out$sims.list$sigma_slope1_prior), col="lightsteelblue3", lwd=2)
hist(out$sims.list$sigma_slope1, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="sigma_slope1")
lines(density(out$sims.list$sigma_slope1), col="lightsteelblue3", lwd=2)

hist(out$sims.list$sigma_slope2_prior, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="sigma_slope2_prior")
lines(density(out$sims.list$sigma_slope2_prior), col="lightsteelblue3", lwd=2)
hist(out$sims.list$sigma_slope2, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="sigma_slope2")
lines(density(out$sims.list$sigma_slope2), col="lightsteelblue3", lwd=2)

hist(out$sims.list$rho_int_slope1_prior, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="rho_int_slope1_prior")
lines(density(out$sims.list$rho_int_slope1_prior), col="lightsteelblue3", lwd=2)
hist(out$sims.list$rho_int_slope1, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="rho_int_slope1")
lines(density(out$sims.list$rho_int_slope1), col="lightsteelblue3", lwd=2)

hist(out$sims.list$rho_int_slope2_prior, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="rho_int_slope2_prior")
lines(density(out$sims.list$rho_int_slope2_prior), col="lightsteelblue3", lwd=2)
hist(out$sims.list$rho_int_slope2, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="rho_int_slope2")
lines(density(out$sims.list$rho_int_slope2), col="lightsteelblue3", lwd=2)

hist(out$sims.list$rho_slope1_slope2_prior, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="rho_slope1_slope2_prior")
lines(density(out$sims.list$rho_slope1_slope2_prior), col="lightsteelblue3", lwd=2)
hist(out$sims.list$rho_slope1_slope2, col="lightsteelblue1", border="white", breaks=10, freq=FALSE, main="rho_slope1_slope2")
lines(density(out$sims.list$rho_slope1_slope2), col="lightsteelblue3", lwd=2)
par(mfrow=c(1, 1))
```






Mark recapture example <https://sourceforge.net/p/mcmc-jags/discussion/610037/thread/bf61fe62/>


Bits I won't use   
 # al[j] <- (mu[j]*mu[j] - mu[j]*mu[j]*mu[j] - mu[j]*sigma*sigma) 
      # be[j] <- (mu[j]- 2*mu[j]*mu[j]+ mu[j]*mu[j]*mu[j] - sigma[j]*sigma[j] + sigma[j]*sigma[j]*mu[j])/sigma[j]*sigma[j]
      
  # for(j in 1:TC){ ## 4 treatments and controls
  # }
<https://sourceforge.net/p/mcmc-jags/discussion/610037/thread/01f7e32c/> beta regression example with mixed effects

Example <https://sourceforge.net/p/mcmc-jags/discussion/610037/thread/2723178a/> 
model {

## Hyperparameters
pi ~ dbeta(.5,.5)
mu ~ dnorm(0, .001)
sd ~ dgamma(.001, .001)
df ~ dgamma(.001, .001)
alpha ~ dunif(0, 100)
beta ~ dunif(0, 100)

## Observation specific
for (i in 1:I) {
  Model.index[i] ~dbern(pi)
  Latent[i,1] ~ dt(mu, sd, df)T(0,1)
  Latent[i,2] ~ dbeta(alpha,beta)
  Data[i] <- Latent[i,Model.index[i]+1] ## Index is 1 based, Bernouli is 0 base.
}
}
